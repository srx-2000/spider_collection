## 知乎百万级用户数据爬虫

​	最近一直在忙考研的事情，所以一直没怎么更新，但这次来了个大的~~，之前有朋友在issue中提出要我研究一下微信公众号的爬虫，这在无形中提醒了我之前想做的一个事情——知乎用户爬虫【之前在做问答爬虫的时候就在考虑这个事情了】。可由于当时的技术限制以及知乎的动态渲染导致进度一直搁置。而这两天由于学校要做一个项目实践，我又重新思考了一下具体的实现方案，终于钻了知乎的一个空子，实现了这个百万级的爬虫。同时此次爬虫我是实实在在的进行了一定构思的，所以也对各个组件进行了分级、注释，所以在代码的可读性和结构性上也比之前要强很多。

​	首先大致说一下我们项目的需求吧。

1. **做一个百万级的用户数据爬虫，获取用户个人主页上的详细信息。**

2. **对着百万数据进行分析，并利用数据清洗工具和可视化工具给出一些的分析图表。**

3. **给使用者提供用户数据检索功能**

4. **给出单日用户人气排行【这里我们有一套自己的人气算法，该人气排行是每天更新热点问题中的活跃用户】**

5. **给用户提供一套可交互ui设计**

6. **使用者可以进入被检索用户的个人主页，下载该用户的全部数据，包括：回答，视频，专栏等【该模块后经讨论已删除】**

​	而我作为本项目的数据开发员，主要担任的任务有：爬虫编写，数据分析，检索功能，以及排行算法的指定等。其中数据可视化由前端开发通过动态渲染达成。同时这里由于我们项目中定义的数据分析周期是1个月一次更新，所以这里我将原先的任务进行了分割，现阶段每日获取的用户数据量大概为5w-10w不等。这样一个月就有了150w的数据。当然后续还有对这个方案进行优化的打算，那个方案应该可以做到每日百万的效果吧。

​	然后就是最主要的这个爬虫的具体功能以及使用方法：

#### 功能

- [x] 获取用户的详细信息【日均5w-10w】
- [ ] ~~获取用户问答，视频等详细数据【暂时取消】~~
- [ ] ~~对上述数据提供下载【暂时取消】~~
- [x] 对获取的数据进行数据清洗以及分析
- [x] 对单个被检索用户进行实时爬取
- [x] 对爬取的数据进行二元组分析【详见[数据分析文件夹](https://github.com/srx-2000/spider_collection/tree/master/zhihu_user_info_spider/data_analysis)下的README】
- [x] 给出四种排行榜算法，并使用算法对用户进行排序选出前20【详见[数据分析文件夹](https://github.com/srx-2000/spider_collection/tree/master/zhihu_user_info_spider/data_analysis)下的README】

#### 使用方法

###### **使用pycharm编译器【推荐】**

* **git clone**

​	`https://github.com/srx-2000/git_spider.git`

* **安装依赖**

  需要使用到的库已经放在requirements.txt，使用pip安装的可以使用指令`pip install -r requirements.txt`。如果国内安装第三方库比较慢，可以使用以下指令进行清华源加速`pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/`

* **配置文件【如果未配置下列文件，则爬虫无法运行】**

	1. 打开`util/util_content.json`文件，配置以下是五个配置项，

     | save_method    | 用于规定保存方式，现仅有csv保存方式                          |
     | -------------- | ------------------------------------------------------------ |
     | **cookies**    | **用来保存个人cookie的，至少需要一个，cookie的获取方式见[连接](https://jingyan.baidu.com/article/5d368d1ea6c6e33f60c057ef.html)** |
     | **account**    | **用来标识上述cookie的，防止建立cookie池时搞混cookie，不填也无所谓，不会影响运行** |
     | **batch_size** | **用来规定爬取多少条数据向csv中保存一次的**                  |
     | **thread_num** | **线程数，一般开20左右就够用了，我这里自己测的时候，20个线程2分钟就能爬到1w的数据了** |
  
   ```json
   {
     "save_method": "csv",
	   "cookies": [
         
     ],
     "account": [
      {
      }
     ],
     "batch_size": 1000,
     "thread_num": 20
   }
   ```
  
  2. 打开`zhihu_user_info\proxypool\config.yaml`配置ip池的host和端口 ，其中ip池的具体使用方式[详见](https://github.com/srx-2000/git_spider/tree/master/proxy_pool)
  
   ```yaml
   # https://github.com/jhao104/proxy_pool
   # 上述项目是本项目使用的代理池，原项目超级棒，而且在使用教学上也可以说是相当详细。
   # 一般来说将上述项目clone下来，在本机运行就可以了，如果想要自己配置一个服务器24小时的更新代理的话也可以将上述项目放到服务器上跑
   # 但要注意如果是放到服务器上跑，记得把下面的host改成自己服务器的公网IP
   
   # 主机地址，如果仅是在本机跑的话就改成127.0.0.1
   host: 127.0.0.1
   # 代理池接口端口，这里使用的是项目的默认端口
   port: 5010
   ```

* **运行爬虫（IP池需要启动详见上面链接）**
  1. 打开爬虫项目运行spider.py文件，点击运行即可。
  2. 因为本模块分为两个子模块分别在QuestionRequester.py和UserRequester.py中。其中QuestionRequester模块主要作用是获取热榜问题，并将热榜上的人的uuid获取到。而UserRequester模块主要作用是通过刚刚爬取的uuid去用户的个人信息出获取真正的个人详细信息。**所以这里可以将两个模块分开运行，优先使用QuestionRequester模块，然后运行UserRequester模块即可**。
  3. 有了上面模块的支持，这里就可以在本项目的基础上添加一个时间表模块，每隔3-4个小时运行QuestionRequester模块去知乎上爬取一波热榜问题的uuid，然后以天为单位到晚上12点再运行UserRequester模块爬取用户的详细信息。

###### **无pycharm编译器**

* **相关配置与上述相同**


* **运行爬虫（IP池需要启动详见上面链接）**

  使用命令行运行：进入项目根目录使用`python spider.py`即可运行

###### 持续部署

* **相关配置与上述相同**


* **运行爬虫（IP池需要启动详见上面链接）**

  使用命令行运行：进入爬虫项目目录下的`scheduler`使用`python go.py 【模式】`即可运行，其中【模式】包括以下几种：

  * hot【每隔两个小时爬取一次知乎热榜】

  * uuid【每天晚上11点爬取今日获取到的用户的uuid】

  * info【每天晚上11点59分根据今日获取到的用户的uuid的爬取详细用户信息】

  > **建议：**直接一次性全部开启【然后就不用管了】，也就是分别运行命令：
  >
  > `python go.py hot`
  >
  > `python go.py uuid`
  >
  > `python go.py info`
  >
  > **关于log：**使用上面的方法运行成功之后，期间生成的相关信息以及报错都可以在`\scheduler\log`文件夹中找到。

#### 运行效果

1. **用户id获取**

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/1.png)

2. **用户信息获取**

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/2.png)

3. **最终结果**

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/3.png)

   ![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/4.png)

#### 爬取思路

​	知乎作为大家熟知的问答社区，也被很多人当成了爬虫练手的基础站。但对于很多人都仅仅停留在对知乎的问答进行爬取，而这个可以说几乎没有难度，因为知乎本身也对这个没有设置什么反爬手段，也就是封个ip而已，随便搭个ip池就解决了。但知乎对于用户数据的防范却比问答多一些。之所以这么说是因为如果想要大批量的爬取用户信息的话无论如何都无法避免对用户的**传播方案**，而传统的传播方案无非就是对一个用户的粉丝和关注进行随机取数，然后不断递归下去。但知乎也料到了这个传播方式，所以使用了动态渲染对用户的关注和粉丝进行了屏蔽。同时如果想要直接通过知乎的接口获取数据就绕不开cookie【需要大量账号】，同时还有各种蜜汁请求头。

​	一开始我也被上述问题给难住了，这也就是为什么这个项目一直搁置的原因。可后来当我跳出这个思维定式之后就有了新的思路：爬取知乎的问答既然不需要cookie，同时也不会有奇怪的请求头，那么我为什么不可以把这个作为我的传播方案呢。既然理论成立，那么就开始实践，最终也证明了我的想法是正确的：每日定时获取当日的热榜信息，并爬取热榜中回答了问题的用户的uuid，再使用这个uuid获取具体的用户信息。这样虽然爬取量无法达到一次性的100w的数据，但却可以保证稳定高效。如果一个ip池中有10个ip的话，平均下来每个ip每天仅仅会请求250次左右，这个请求数量甚至可以让知乎都意识不到我是一个爬虫【知乎短时间的请求数达到500才会触发验证机制】，而这仅仅是10个ip的ip池，如果将池子扩大，同时增加一定的间隔，甚至可以做到24小时不间断无限爬取【但其实热榜的回答数量有限所以意义不大】。同时这个方案另一个很棒的点是对于cookie的消费会极小，因为cookie只会使用在对热榜的获取上，而这个请求其实可以几个小时进行一次，完全在一个真实用户的操作范围之内。

#### 更新日志

* 2021.10.17

  更新第一版，完成每日用户详细信息获取
  
* 2021.11.17

  更新第二版爬虫【添加了一些配置项】，以及数据分析模块
  
* 2021.12.29

  更新第三版爬虫

  ​	**Add**

  1. 添加持续部署模块【schedule模块】，实现在服务器上持续爬取

  ​	**Fix**

  1. 修复代理池无法实现高匿名导致的本机ip暴露问题
  
* 2022.1.3

  更新第三版爬虫

  **Fix**

  1. 修复了因添加持续部署模块而无法使用Requester模块直接运行的bug
  2. 修复了在获取到的csv数据中存在的apschedule相关数据列而导致数据出错的bug

  